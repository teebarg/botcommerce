# ğŸ¤– Customer Support Agent

An agentic AI customer support system built with **LangChain**, **Qdrant**, **RAG**, and **FastAPI**.
Runs on Groq â€” deployable on 500MB free tier servers.

---

## ğŸ—ï¸ Architecture

```
Your App
      â”‚
      â–¼ POST /chat
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI (Render 500MB)    â”‚
â”‚                             â”‚
â”‚   LangChain ReAct Agent     â”‚
â”‚   â”œâ”€â”€ search_products       â”‚â”€â”€â–º Qdrant (free cloud)
â”‚   â”œâ”€â”€ search_faqs           â”‚â”€â”€â–º Qdrant (free cloud)
â”‚   â”œâ”€â”€ search_policies       â”‚â”€â”€â–º Qdrant (free cloud)
â”‚   â”œâ”€â”€ check_order_status    â”‚â”€â”€â–º shop API
â”‚   â”œâ”€â”€ check_stock           â”‚â”€â”€â–º shop API
â”‚   â”œâ”€â”€ request_refund        â”‚â”€â”€â–º shop API
â”‚   â””â”€â”€ escalate_to_human     â”‚â”€â”€â–º Human Helpdesk
â”‚                             â”‚
â”‚   LLM: Groq (prod)          â”‚
â”‚   Memory: Redis (session)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Quick Start (Local Development)

### Prerequisites
- Python 3.11+
- Docker (optional, for Redis)

### 1. Clone & Install
```bash
git clone <your-repo>
cd customer-support-agent
pip install -r requirements.txt
```

### 2. Set Up Environment
```bash
cp .env.example .env
```

### 3. Start Redis (with Docker)
```bash
docker run -d -p 6379:6379 redis:alpine
```

### 4. Ingest Your Data
```bash
# Load all collections (products, FAQs, policies)
python -m app.rag.ingest --collection all

# Or load individually
python -m app.rag.ingest --collection products
python -m app.rag.ingest --collection faqs
```

### 5. Start the Server
```bash
uvicorn app.main:app --reload --port 8000
```

### 6. Test It
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Where is my order #12345?", "session_id": "test-session"}'
```

Visit **http://localhost:8000/docs** for the interactive API documentation.

---

## ğŸš€ Deployment (Render Free Tier)

### Prerequisites
- [Qdrant Cloud](https://cloud.qdrant.io) free account (1GB, no credit card)
- [Groq](https://console.groq.com) free API key
- [Render](https://render.com) account

### Environment Variables on Render
Set these in your Render service settings:

| Variable | Value |
|---|---|
| `ENV` | `production` |
| `GROQ_API_KEY` | From console.groq.com |
| `QDRANT_URL` | From cloud.qdrant.io |
| `QDRANT_API_KEY` | From cloud.qdrant.io |
| `REDIS_URL` | From Render Redis service |
| `API_BASE_URL` | Your shop API URL |

### Deploy Steps
1. Push your code to GitHub
2. Create a new **Web Service** on Render â†’ connect your repo
3. Set **Build Command**: `pip install -r requirements.txt`
4. Set **Start Command**: `uvicorn app.main:app --host 0.0.0.0 --port 8000`
5. Add all environment variables
6. Deploy!

### Ingest Data After Deployment
```bash
curl -X POST https://your-render-app.onrender.com/ingest \
  -H "Content-Type: application/json" \
  -d '{"collection": "all"}'
```

---

## ğŸ“ Project Structure

```
customer-support-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point & endpoints
â”‚   â”œâ”€â”€ config.py            # Settings & LLM factory (Groq)
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ agent.py         # LangChain ReAct agent loop
â”‚   â”‚   â”œâ”€â”€ tools.py         # All agent tools (RAG + shop API)
â”‚   â”‚   â””â”€â”€ memory.py        # Redis-backed conversation memory
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ qdrant_client.py # Qdrant connection & search
â”‚   â”‚   â””â”€â”€ ingest.py        # Data ingestion pipeline
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ models.py        # Pydantic v2 request/response models
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products.json        # Your product catalog
â”‚   â”œâ”€â”€ faqs.json            # Frequently asked questions
â”‚   â””â”€â”€ policies.txt         # Store policies
â”œâ”€â”€ Dockerfile               # Optimized for Render 500MB
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

---

## ğŸ”Œ Integrating with Your Shop

Send a POST request to `/chat` for every customer message:

```javascript
// In your frontend
const response = await fetch('https://api.agent.com/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    message: userMessage,
    session_id: sessionStorage.getItem('supportSessionId'), // persist across messages
    customer_id: loggedInUser?.id,  // optional: pass if user is logged in
  })
});

const data = await response.json();
// data.reply        â†’ show this to the customer
// data.session_id   â†’ save this for the next message
// data.escalated    â†’ if true, notify a human agent
```

---

## ğŸ› ï¸ Updating Your Data

Whenever you add new products, FAQs, or update policies:

1. Call the ingest endpoint:
```bash
curl -X POST https://your-agent.onrender.com/ingest \
  -d '{"collection": "products"}'
```

---

## ğŸ“Š CV Skills This Project Demonstrates

- **LangChain** â€” ReAct agents, tool calling, memory management
- **RAG** (Retrieval-Augmented Generation) â€” semantic search over knowledge base
- **Qdrant** â€” vector database, embedding storage, semantic search
- **Sentence Transformers** â€” local embeddings, `all-MiniLM-L6-v2`
- **FastAPI** â€” async REST API, background tasks, middleware
- **Pydantic v2** â€” data validation and settings management
- **Redis** â€” distributed session state and conversation memory
- **Docker** â€” containerization, multi-stage builds, health checks
- **Prompt Engineering** â€” ReAct format, agent behavior control
- **Model-agnostic design** â€” Claude / Groq switching
- **Groq API** â€” LLM inference, open-source model deployment
